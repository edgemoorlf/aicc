# AIå‚¬æ”¶åŠ©æ‰‹ - Chrome WebM/Opuså®ç°æœåŠ¡å™¨
# æ”¯æŒWebMæ ¼å¼ï¼Œé€šè¿‡WebMâ†’WAVè½¬æ¢åå‘é€åˆ°DashScope
# é’ˆå¯¹Chromeã€Edgeã€Operaæµè§ˆå™¨ä¼˜åŒ–

import os
import re
import json
import base64
import time
import requests
import requests
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv
import dashscope
from dashscope import Generation
from dashscope.audio.asr import Recognition
import logging

# Load environment variables from .env file
load_dotenv()

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# é…ç½®
DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')
if not DASHSCOPE_API_KEY:
    logger.error('DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®')
    raise ValueError('è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡')

dashscope.api_key = DASHSCOPE_API_KEY

# å…¨å±€å˜é‡
conversation_history = []
active_asr_sessions = {}  # å­˜å‚¨æ´»è·ƒçš„æµå¼ASRä¼šè¯
client_voice_settings = {}  # å­˜å‚¨å®¢æˆ·ç«¯è¯­éŸ³è®¾ç½®

# é™æ€æ–‡ä»¶æœåŠ¡
@app.route('/')
def index():
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory('.', filename)

def merge_wav_audio(wav_segments):
    """åˆå¹¶å¤šä¸ªWAVéŸ³é¢‘æ®µè½ä¸ºä¸€ä¸ªå®Œæ•´çš„WAVæ–‡ä»¶"""
    if not wav_segments:
        return []
    
    if len(wav_segments) == 1:
        return wav_segments[0]
    
    import struct
    
    # æ”¶é›†æ‰€æœ‰PCMæ•°æ®ï¼ˆè·³è¿‡æ¯ä¸ªWAVçš„44å­—èŠ‚å¤´ï¼‰
    all_pcm_data = b''
    
    for wav_data in wav_segments:
        if len(wav_data) > 44:  # ç¡®ä¿æœ‰WAVå¤´
            pcm_data = bytes(wav_data[44:])  # è·³è¿‡44å­—èŠ‚çš„WAVå¤´
            all_pcm_data += pcm_data
        else:
            # å¦‚æœæ•°æ®å¤ªçŸ­ï¼Œå½“ä½œPCMæ•°æ®å¤„ç†
            all_pcm_data += bytes(wav_data)
    
    # åˆ›å»ºæ–°çš„WAVå¤´ç”¨äºåˆå¹¶çš„éŸ³é¢‘
    sample_rate = 24000  # 24kHzï¼ˆä¸DashScope TTSä¸€è‡´ï¼‰
    num_channels = 1     # å•å£°é“
    bits_per_sample = 16 # 16ä½PCM
    data_size = len(all_pcm_data)
    file_size = 36 + data_size
    
    # åˆ›å»ºWAVæ–‡ä»¶å¤´
    header = bytearray(44)
    
    # RIFF chunk
    header[0:4] = b'RIFF'
    struct.pack_into('<I', header, 4, file_size)
    header[8:12] = b'WAVE'
    
    # fmt chunk
    header[12:16] = b'fmt '
    struct.pack_into('<I', header, 16, 16)  # fmt chunk size
    struct.pack_into('<H', header, 20, 1)   # PCM format
    struct.pack_into('<H', header, 22, num_channels)
    struct.pack_into('<I', header, 24, sample_rate)
    struct.pack_into('<I', header, 28, sample_rate * num_channels * bits_per_sample // 8)  # byte rate
    struct.pack_into('<H', header, 32, num_channels * bits_per_sample // 8)  # block align
    struct.pack_into('<H', header, 34, bits_per_sample)
    
    # data chunk
    header[36:40] = b'data'
    struct.pack_into('<I', header, 40, data_size)
    
    # åˆå¹¶å¤´éƒ¨å’ŒPCMæ•°æ®
    merged_wav = bytes(header) + all_pcm_data
    
    logger.info(f'åˆå¹¶WAVéŸ³é¢‘: {len(wav_segments)}ä¸ªæ®µè½ -> {len(merged_wav)} bytes')
    return list(merged_wav)

def segment_ai_response(ai_text):
    """å°†AIå›å¤æŒ‰ç…§å‚¬æ”¶å‘˜æ ‡è®°åˆ†æ®µï¼Œå¹¶æ¸…ç†æ ¼å¼"""
    import re
    
    # åˆ†å‰²æ–‡æœ¬ï¼ŒæŸ¥æ‰¾ "å‚¬æ”¶å‘˜:" æ¨¡å¼ï¼ˆå¯èƒ½å‰é¢æœ‰æ•°å­—ç¼–å·ï¼‰
    # åŒ¹é…æ¨¡å¼ï¼šå¯é€‰çš„æ•°å­—å’Œç‚¹ï¼Œç„¶åæ˜¯"å‚¬æ”¶å‘˜:"
    segments = re.split(r'\d*\.?\s*å‚¬æ”¶å‘˜[ï¼š:]\s*', ai_text)
    
    # è¿‡æ»¤æ‰ç©ºæ®µè½å¹¶æ¸…ç†
    clean_segments = []
    for segment in segments:
        segment = segment.strip()
        if segment:  # åªä¿ç•™éç©ºæ®µè½
            clean_segments.append(segment)
    
    logger.info(f'AIå›å¤åˆ†æ®µ: åŸæ–‡é•¿åº¦={len(ai_text)}, åˆ†æ®µæ•°={len(clean_segments)}')
    for i, seg in enumerate(clean_segments):
        logger.info(f'æ®µè½{i+1}: "{seg[:50]}..."')
    
    return clean_segments

# APIç«¯ç‚¹ï¼šå‘é€æ¶ˆæ¯å¹¶è·å–éŸ³é¢‘å“åº”
@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        message = data.get('message', '')
        message_type = data.get('messageType', 'user')
        customer_context = data.get('customerContext', {})
        conversation_history = data.get('conversationHistory', [])
        
        logger.info(f'æ”¶åˆ°æ¶ˆæ¯: {message[:50]}... ç±»å‹: {message_type}')
        
        # å¯¹äºä»£ç†é—®å€™è¯­ï¼Œç›´æ¥ä½¿ç”¨TTS
        if message_type == 'agent_greeting':
            audio_data = generate_tts_audio(message)
            return jsonify({
                'audio': audio_data,
                'text': message
            })
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        system_prompt = build_collection_prompt(customer_context, conversation_history)
        
        # è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆå›å¤
        ai_response = generate_ai_response(system_prompt, message)
        
        if not ai_response:
            return jsonify({'error': 'ç”ŸæˆAIå›å¤å¤±è´¥'}), 500
        
        # å°†AIå›å¤åˆ†æ®µ
        segments = segment_ai_response(ai_response)
        
        if not segments:
            # å¦‚æœæ²¡æœ‰åˆ†æ®µï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡
            segments = [ai_response]
        
        # ä¸ºç¬¬ä¸€ä¸ªæ®µè½ç”Ÿæˆè¯­éŸ³å¹¶ç«‹å³è¿”å›ï¼ˆä½å»¶è¿Ÿä¼˜å…ˆï¼‰
        if segments:
            first_segment = segments[0]
            logger.info(f'ç”Ÿæˆç¬¬1æ®µè¯­éŸ³ï¼ˆä¼˜å…ˆè¿”å›ï¼‰: {first_segment[:30]}...')
            first_audio = generate_tts_audio(first_segment)
            
            # å¦‚æœæœ‰å¤šä¸ªæ®µè½ï¼Œè®°å½•å‰©ä½™æ®µè½ï¼ˆå¯ä»¥åç»­é€šè¿‡å…¶ä»–æœºåˆ¶å¤„ç†ï¼‰
            if len(segments) > 1:
                logger.info(f'å‰©ä½™{len(segments)-1}ä¸ªæ®µè½å°†è¢«è·³è¿‡ï¼Œä¼˜å…ˆä½å»¶è¿Ÿå“åº”')
                # TODO: å¯ä»¥è€ƒè™‘é€šè¿‡WebSocketæˆ–å…¶ä»–æœºåˆ¶å‘é€å‰©ä½™æ®µè½
            
            logger.info(f'AIå›å¤: {ai_response[:50]}... (ç«‹å³è¿”å›ç¬¬1æ®µï¼Œå…±{len(segments)}æ®µ)')
            
            return jsonify({
                'audio': first_audio if first_audio else [],
                'text': ai_response  # è¿”å›å®Œæ•´æ–‡æœ¬ç”¨äºæ˜¾ç¤º
            })
        else:
            # æ²¡æœ‰åˆ†æ®µï¼Œè¿”å›ç©ºéŸ³é¢‘
            logger.warning('æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„AIå›å¤æ®µè½')
            return jsonify({
                'audio': [],
                'text': ai_response
            })
        
    except Exception as e:
        logger.error(f'èŠå¤©å¤„ç†é”™è¯¯: {str(e)}')
        return jsonify({'error': f'å¤„ç†å¤±è´¥: {str(e)}'}), 500

# APIç«¯ç‚¹ï¼šè¯­éŸ³è½¬æ–‡å­—
@app.route('/api/transcribe', methods=['POST'])
def transcribe():
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰æ¥æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶'}), 400
            
        audio_file = request.files['audio']
        logger.info(f'å¼€å§‹è½¬å½•éŸ³é¢‘ï¼Œå¤§å°: {len(audio_file.read())} bytes')
        audio_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        # ä½¿ç”¨DashScopeè¯­éŸ³è¯†åˆ«API
        transcript = recognize_speech_dashscope(audio_file)
        
        if transcript:
            logger.info(f'è½¬å½•å®Œæˆ: {transcript}')
            return jsonify({
                'transcript': transcript,
                'confidence': 0.95
            })
        else:
            logger.error('è¯­éŸ³è¯†åˆ«å¤±è´¥')
            return jsonify({
                'error': 'è¯­éŸ³è¯†åˆ«å¤±è´¥',
                'transcript': '',  # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯é”™è¯¯
                'confidence': 0.0
            }), 200  # è¿”å›200ä½†ç½®ä¿¡åº¦ä¸º0
        
    except Exception as e:
        logger.error(f'éŸ³é¢‘è½¬å½•å¤±è´¥: {str(e)}')
        return jsonify({
            'error': 'éŸ³é¢‘è½¬å½•å¤±è´¥',
            'details': str(e),
            'transcript': '',  # æä¾›ç©ºçš„è½¬å½•ç»“æœ
            'confidence': 0.0
        }), 200  # è¿”å›200è®©å®¢æˆ·ç«¯å¯ä»¥ç»§ç»­å¤„ç†

# APIç«¯ç‚¹ï¼šå‡†ç¡®æ€§è¯„ä¼°
@app.route('/api/evaluate-accuracy', methods=['POST'])
def evaluate_accuracy():
    try:
        data = request.get_json()
        original_text = data.get('originalText', '')
        spoken_text = data.get('spokenText', '')
        context = data.get('context', 'é“¶è¡Œå‚¬æ”¶å¯¹è¯')
        
        logger.info(f'è¯„ä¼°å‡†ç¡®æ€§: åŸæ–‡é•¿åº¦={len(original_text)}, è¯†åˆ«é•¿åº¦={len(spoken_text)}')
        
        # ä½¿ç”¨é€šä¹‰åƒé—®è¿›è¡Œå‡†ç¡®æ€§è¯„ä¼°
        evaluation = evaluate_transcript_accuracy(original_text, spoken_text, context)
        
        logger.info(f'è¯„ä¼°å®Œæˆ: {evaluation.get("overall_score", 0)}åˆ†')
        
        return jsonify(evaluation)
        
    except Exception as e:
        logger.error(f'å‡†ç¡®æ€§è¯„ä¼°å¤±è´¥: {str(e)}')
        return jsonify({
            'error': 'å‡†ç¡®æ€§è¯„ä¼°å¤±è´¥',
            'details': str(e)
        }), 500

def build_collection_prompt(customer_context, conversation_history):
    """æ„å»ºå‚¬æ”¶ä¸“å‘˜çš„ç³»ç»Ÿæç¤º"""
    
    # æ ¼å¼åŒ–é‡‘é¢
    def format_chinese_amount(amount):
        if amount >= 10000:
            wan = amount // 10000
            remainder = amount % 10000
            if remainder == 0:
                return f"{wan}ä¸‡å…ƒ"
            else:
                return f"{wan}ä¸‡{remainder}å…ƒ"
        return f"{amount}å…ƒ"
    
    # æ„å»ºå¯¹è¯å†å²ï¼ˆä¸åŒ…å«å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
    conversation_text = ""
    if conversation_history:
        conversation_text = "\næœ¬æ¬¡é€šè¯è®°å½•:\n"
        for i, entry in enumerate(conversation_history):
            role = "å®¢æˆ·" if entry.get('sender') == 'user' else "å‚¬æ”¶å‘˜"
            conversation_text += f"{i+1}. {role}: {entry.get('text', '')}\n"
    else:
        conversation_text = "\næœ¬æ¬¡é€šè¯è®°å½•:\n[å¯¹è¯åˆšå¼€å§‹]"
    
    system_prompt = f"""ä½ æ˜¯å¹³å®‰é“¶è¡Œä¿¡ç”¨å¡ä¸­å¿ƒçš„ä¸“ä¸šå‚¬æ”¶ä¸“å‘˜ï¼Œæ­£åœ¨è¿›è¡Œç”µè¯å‚¬æ”¶å·¥ä½œã€‚

å®¢æˆ·æ¡£æ¡ˆä¿¡æ¯:
- å®¢æˆ·å§“å: {customer_context.get('name', 'å®¢æˆ·')}
- é€¾æœŸæœ¬é‡‘: {format_chinese_amount(customer_context.get('balance', 0))}
- é€¾æœŸå¤©æ•°: {customer_context.get('daysOverdue', 0)}å¤©
- è”ç³»å†å²: {customer_context.get('previousContacts', 0)}æ¬¡
- é£é™©ç­‰çº§: {customer_context.get('riskLevel', 'ä¸­ç­‰')}

{conversation_text}

åŸºäºçœŸå®å‚¬æ”¶å¯¹è¯çš„æ ‡å‡†è¯æœ¯:

ã€æ ¸å®ç¡®è®¤ã€‘
- "æˆ‘çœ‹æ‚¨è¿™è¾¹çš„è¯åœ¨[æ—¥æœŸ]è¿˜äº†ä¸€ç¬”ï¼Œè¿˜äº†[é‡‘é¢]"
- "å½“å‰çš„è¯è¿˜å·®[å…·ä½“é‡‘é¢]ï¼Œæ²¡æœ‰è¿˜å¤Ÿ"

ã€ç†è§£å›åº”ã€‘  
- "ä¹Ÿæ²¡æœ‰äººè¯´æœ‰é’±ä¸å»è¿˜è¿™ä¸ªä¿¡ç”¨å¡çš„ï¼Œæˆ‘å¯ä»¥ç†è§£"
- "å¯ä»¥ç†è§£ï¼Œæ‚¨çš„è¿˜æ¬¾å‹åŠ›ç¡®å®ä¹Ÿæ˜¯æŒºå¤§çš„"

ã€æ–¹æ¡ˆæä¾›ã€‘
- "å½“å‰çš„è¯è¿˜æ˜¯å±äºä¸€ä¸ªå†…éƒ¨åå•†"
- "é“¶è¡Œè¿™è¾¹å¯ä»¥å¸®æ‚¨å‡å…ä¸€éƒ¨åˆ†æ¯è´¹"
- "è¿˜å¯ä»¥å¸®æ‚¨å»æ’¤é”€è¿™ä¸ªä½™è–ªæ¡ˆä»¶çš„"

ã€ä¸“ä¸šç”¨è¯­ã€‘
- ä½¿ç”¨"æ‚¨è¿™è¾¹çš„è¯"ã€"å½“å‰çš„è¯"ã€"æ˜¯å§"ç­‰çœŸå®å‚¬æ”¶ç”¨è¯­
- ä½¿ç”¨"å†…éƒ¨åå•†"ã€"ä½™è–ªæ¡ˆä»¶"ã€"å…¨é¢å‡å…æ–¹æ¡ˆæ”¿ç­–"ç­‰ä¸“ä¸šæœ¯è¯­

ã€é‡è¦åŸåˆ™ã€‘
1. ä¿æŒç†è§£è€å¿ƒçš„æ€åº¦ï¼Œé¿å…å¼ºç¡¬æ–½å‹
2. ç”¨å…·ä½“æ•°æ®å»ºç«‹å¯ä¿¡åº¦  
3. æä¾›å¤šç§è§£å†³æ–¹æ¡ˆ
4. å…³æ³¨å®¢æˆ·æ„Ÿå—å’Œå®é™…å›°éš¾
5. ä½¿ç”¨é“¶è¡Œä¸“ä¸šæœ¯è¯­å¢å¼ºæƒå¨æ€§
6. æ¯ä¸€æ¬¡å›ç­”å°½é‡ç®€ç»ƒï¼Œä¸è¦è¶…è¿‡4å¥è¯ï¼Œæœ€å¥½åœ¨1-2å¥ï¼Œé¿å…é•¿ç¯‡å¤§è®ºï¼Œç¡®ä¿å®¢æˆ·èƒ½å¬æ‡‚
7. **ä¸¥ç¦é‡å¤ä¹‹å‰å·²ç»è¯´è¿‡çš„å†…å®¹** - ä»”ç»†æŸ¥çœ‹é€šè¯è®°å½•ï¼Œé¿å…é‡å¤ç›¸åŒçš„è¯æœ¯ã€é—®é¢˜æˆ–ä¿¡æ¯
8. **æ ¹æ®å¯¹è¯è¿›å±•è°ƒæ•´ç­–ç•¥** - æ¯æ¬¡å›å¤éƒ½è¦åŸºäºå®¢æˆ·çš„æœ€æ–°å›åº”ï¼Œæ¨è¿›å¯¹è¯è€Œä¸æ˜¯é‡å¤

ã€é˜²é‡å¤æŒ‡å—ã€‘
- å¦‚æœå®¢æˆ·å·²ç»è¡¨è¾¾äº†æŸç§æ€åº¦æˆ–ç«‹åœºï¼Œä¸è¦é‡å¤è¯¢é—®ç›¸åŒçš„é—®é¢˜
- å¦‚æœå·²ç»æåˆ°è¿‡æŸç§è§£å†³æ–¹æ¡ˆï¼Œä¸è¦å†æ¬¡é‡å¤ä»‹ç»
- æ ¹æ®å®¢æˆ·çš„å…·ä½“å›åº”ï¼Œé€‰æ‹©æ–°çš„è§’åº¦æˆ–æ›´æ·±å…¥çš„æ¢è®¨
- é¿å…ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å¼€åœºç™½æˆ–ç»“æŸè¯­

è¯­è¨€è¦æ±‚:
- ä½¿ç”¨å¤§é™†æ ‡å‡†æ™®é€šè¯ï¼Œé¿å…å°æ¹¾ç”¨è¯­
- é‡‘é¢è¡¨è¾¾: 15000å…ƒè¯´æˆ"ä¸€ä¸‡äº”åƒå…ƒ"ï¼Œä¸æ˜¯"åäº”åƒå…ƒ"
- è¯­æ°”è¦ä¸“ä¸šã€ç†è§£ï¼Œä½“ç°äººæ–‡å…³æ€€

è¯·ä»”ç»†åˆ†æå®Œæ•´çš„é€šè¯è®°å½•ï¼Œç¡®ä¿ä¸é‡å¤ä¹‹å‰çš„å†…å®¹ï¼Œä»¥ä¸“ä¸šå‚¬æ”¶å‘˜çš„èº«ä»½é’ˆå¯¹å®¢æˆ·æœ€æ–°è¯è¯­ç»™å‡ºæ–°çš„ã€æœ‰è¿›å±•çš„å›åº”ã€‚"""

    return system_prompt

def generate_ai_response(system_prompt, user_message):
    """ä½¿ç”¨é€šä¹‰åƒé—®ç”ŸæˆAIå›å¤"""
    try:
        llm_start_time = time.time()

        response = Generation.call(
            model='qwen-plus',
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_message}
            ],
            temperature=0.7,
            max_tokens=500,
            result_format='message'
        )

        llm_latency = int((time.time() - llm_start_time) * 1000)

        if response.status_code == 200:
            ai_text = response.output.choices[0].message.content
            return ai_text.strip(), llm_latency
        else:
            logger.error(f'é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥: {response.status_code}')
            return None, 0

    except Exception as e:
        logger.error(f'ç”ŸæˆAIå›å¤é”™è¯¯: {str(e)}')
        return None, 0

def generate_tts_audio_streaming(text, segment_index=0, total_segments=1, voice_settings=None):
    """ä½¿ç”¨é€šä¹‰åƒé—®TTSç”Ÿæˆè¯­éŸ³ï¼Œå®æ—¶æµå¼å‘é€PCMæ•°æ®"""
    max_retries = 3

    # è·å–è¯­éŸ³è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if voice_settings is None:
        voice_settings = {}
    voice = voice_settings.get('voice', 'Dylan')
    speed = voice_settings.get('speed', 1.0)
    pitch = voice_settings.get('pitch', 1.0)
    volume = voice_settings.get('volume', 0.8)

    for attempt in range(max_retries):
        try:
            logger.info(f'å¼€å§‹æµå¼TTSéŸ³é¢‘ç”Ÿæˆ (å°è¯• {attempt + 1}/{max_retries}): {text[:30]}... å£°éŸ³: {voice}')

            tts_start_time = time.time()

            # ä½¿ç”¨æµå¼æ–¹å¼ï¼Œç”ŸæˆPCMæ•°æ®æµ
            responses = dashscope.audio.qwen_tts.SpeechSynthesizer.call(
                model="qwen-tts-latest",
                api_key=DASHSCOPE_API_KEY,
                text=text,
                voice=voice,
                stream=True
            )

            # æ£€æŸ¥responsesæ˜¯å¦ä¸ºNone
            if responses is None:
                raise ValueError("TTS APIè¿”å›Noneå“åº”")

            # å®æ—¶æµå¼å‘é€PCMæ•°æ®å—
            chunk_count = 0
            first_chunk_time = None

            for chunk in responses:
                if chunk and "output" in chunk and "audio" in chunk["output"] and "data" in chunk["output"]["audio"]:
                    audio_string = chunk["output"]["audio"]["data"]
                    pcm_bytes = base64.b64decode(audio_string)
                    if pcm_bytes:
                        chunk_count += 1

                        # è®°å½•ç¬¬ä¸€ä¸ªå—çš„æ—¶é—´ï¼ˆTTSé¦–æ¬¡å“åº”å»¶è¿Ÿï¼‰
                        if first_chunk_time is None:
                            first_chunk_time = time.time()
                            tts_first_chunk_latency = int((first_chunk_time - tts_start_time) * 1000)

                        logger.info(f'æµå¼å‘é€TTS PCMæ•°æ®å— {chunk_count}: {len(pcm_bytes)} bytes')

                        # ç«‹å³é€šè¿‡WebSocketå‘é€PCMæ•°æ®å—
                        socketio.emit('pcm_chunk', {
                            'pcm_data': list(pcm_bytes),
                            'chunk_index': chunk_count,
                            'segment_index': segment_index,
                            'total_segments': total_segments,
                            'text': text,
                            'sample_rate': 24000,  # DashScope TTSè¾“å‡º24kHz
                            'channels': 1,
                            'bits_per_sample': 16,
                            'volume': volume
                        })
            
            if chunk_count > 0:
                tts_total_latency = int((time.time() - tts_start_time) * 1000)
                logger.info(f'æµå¼TTSå®Œæˆ (å°è¯• {attempt + 1}): å‘é€äº†{chunk_count}ä¸ªPCMæ•°æ®å—')
                logger.info(f'TTSå»¶è¿ŸæŒ‡æ ‡ - é¦–å—: {tts_first_chunk_latency if first_chunk_time else 0}ms, æ€»è®¡: {tts_total_latency}ms')
                
                # å‘é€æ®µè½ç»“æŸä¿¡å·
                socketio.emit('pcm_segment_end', {
                    'segment_index': segment_index,
                    'total_segments': total_segments,
                    'chunk_count': chunk_count
                })
                
                # è¿”å›TTSé¦–æ¬¡å“åº”å»¶è¿Ÿï¼ˆç”¨äºå»¶è¿ŸæŒ‡æ ‡ï¼‰
                return tts_first_chunk_latency if first_chunk_time else tts_total_latency
            else:
                raise ValueError("TTSå“åº”ä¸­æ²¡æœ‰éŸ³é¢‘æ•°æ®")
                
        except Exception as e:
            logger.error(f'æµå¼TTSç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}')
            if attempt == max_retries - 1:
                logger.error('æµå¼TTSé‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œç”Ÿæˆå¤±è´¥')
                return False
            else:
                logger.info(f'ç­‰å¾…1ç§’åé‡è¯•...')
                time.sleep(1)

def generate_tts_audio(text):
    """å…¼å®¹æ€§å‡½æ•°ï¼šä½¿ç”¨é€šä¹‰åƒé—®TTSç”Ÿæˆè¯­éŸ³ï¼Œè¿”å›PCMæ•°æ®å—åˆ—è¡¨"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            logger.info(f'ç”ŸæˆTTSéŸ³é¢‘ (å°è¯• {attempt + 1}/{max_retries}): {text[:30]}...')

            # ä½¿ç”¨æµå¼æ–¹å¼ï¼Œç”ŸæˆPCMæ•°æ®æµ
            responses = dashscope.audio.qwen_tts.SpeechSynthesizer.call(
                model="qwen-tts-latest",
                api_key=DASHSCOPE_API_KEY,
                text=text,
                voice="Dylan",
                stream=True
            )

            # æ£€æŸ¥responsesæ˜¯å¦ä¸ºNone
            if responses is None:
                raise ValueError("TTS APIè¿”å›Noneå“åº”")
            
            # æµå¼è¿”å›PCMæ•°æ®å—
            pcm_chunks = []
            for chunk in responses:
                if chunk and "output" in chunk and "audio" in chunk["output"] and "data" in chunk["output"]["audio"]:
                    audio_string = chunk["output"]["audio"]["data"]
                    pcm_bytes = base64.b64decode(audio_string)
                    if pcm_bytes:
                        pcm_chunks.append(list(pcm_bytes))
                        logger.info(f'TTS PCMæ•°æ®å—: {len(pcm_bytes)} bytes')
            
            if pcm_chunks:
                logger.info(f'TTSéŸ³é¢‘ç”ŸæˆæˆåŠŸ (å°è¯• {attempt + 1}): æ€»å…±{len(pcm_chunks)}ä¸ªPCMæ•°æ®å—')
                return pcm_chunks  # è¿”å›PCMæ•°æ®å—åˆ—è¡¨
            else:
                raise ValueError("TTSå“åº”ä¸­æ²¡æœ‰éŸ³é¢‘æ•°æ®")
                
        except Exception as e:
            logger.error(f'TTSç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}')
            if attempt == max_retries - 1:
                logger.error('TTSé‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œç”Ÿæˆå¤±è´¥')
                return []
            else:
                logger.info(f'ç­‰å¾…1ç§’åé‡è¯•...')
                time.sleep(1)

def recognize_speech_dashscope(audio_file):
    """ä½¿ç”¨DashScopeè¿›è¡Œè¯­éŸ³è¯†åˆ« - æ”¯æŒWebM/Opusæ ¼å¼"""
    try:
        logger.info('å¼€å§‹DashScopeè¯­éŸ³è¯†åˆ«...')
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶å†…å®¹
        audio_content = audio_file.read()
        logger.info(f'éŸ³é¢‘æ–‡ä»¶å¤§å°: {len(audio_content)} bytes')
        
        # ğŸ¯ ç›´æ¥ä½¿ç”¨WebM/Opusæ ¼å¼è¿›è¡ŒASR
        import tempfile
        
        # ä¿å­˜åŸå§‹WebM/Opusæ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as webm_file:
            webm_file.write(audio_content)
            webm_file_path = webm_file.name
        
        logger.info(f'WebMæ–‡ä»¶å¤§å°: {len(audio_content)} bytes location: {webm_file_path}')

        # ğŸ¯ DashScope ASRä¸æ”¯æŒWebMæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºWAV
        # æ”¯æŒçš„æ ¼å¼: pcm, wav, mp3, opus, speex, aac, amr
        try:
            logger.info('è½¬æ¢WebMåˆ°WAVæ ¼å¼è¿›è¡ŒASRè¯†åˆ«...')

            # è½¬æ¢WebMåˆ°8kHz WAV
            from pydub import AudioSegment
            wav_file_path = webm_file_path.replace('.webm', '_8khz.wav')

            audio = AudioSegment.from_file(webm_file_path, format="webm")
            audio = audio.set_frame_rate(8000).set_channels(1).set_sample_width(2)
            audio.export(wav_file_path, format="wav")

            logger.info(f'WAVè½¬æ¢å®Œæˆ: {wav_file_path}')

            # ä½¿ç”¨8kHzæ¨¡å‹è¿›è¡Œè¯†åˆ«
            recognition = Recognition(
                model='paraformer-realtime-8k-v2',
                format='wav',
                sample_rate=8000,
                callback=None,
                # ğŸ¯ é«˜çº§å‚æ•°ä¼˜åŒ–
                semantic_punctuation_enabled=True,  # æ™ºèƒ½æ ‡ç‚¹ç¬¦å·
                max_sentence_silence=2000,          # 2ç§’é™éŸ³æ£€æµ‹ï¼Œé€‚åº”è‡ªç„¶å¯¹è¯
                heartbeat=True                      # å¿ƒè·³ä¿æŒé•¿è¿æ¥ç¨³å®š
            )

            # è¿›è¡Œè¯­éŸ³è¯†åˆ«
            result = recognition.call(wav_file_path)
            logger.info(f'ASRè°ƒç”¨å®Œæˆï¼Œç»“æœçŠ¶æ€: {getattr(result, "status_code", "æœªçŸ¥")}')

            if hasattr(result, 'get_sentence') and result.get_sentence():
                sentences = result.get_sentence()
                logger.info(f'è¯†åˆ«åˆ° {len(sentences)} ä¸ªå¥å­')

                # åˆå¹¶æ‰€æœ‰å¥å­çš„æ–‡æœ¬
                transcript_parts = []
                for sentence_obj in sentences:
                    if isinstance(sentence_obj, dict) and 'text' in sentence_obj:
                        transcript_parts.append(sentence_obj['text'])

                transcript = ''.join(transcript_parts)
                logger.info(f'è¯†åˆ«ç»“æœ: {transcript}')
                return transcript.strip()

            elif hasattr(result, 'output') and result.output:
                logger.info(f'ASR outputç±»å‹: {type(result.output)}')
                logger.info(f'ASR outputå†…å®¹: {result.output}')

                # å°è¯•ä»outputä¸­è·å–ç»“æœ
                if hasattr(result.output, 'sentence') and result.output.sentence:
                    sentences = result.output.sentence
                    transcript_parts = []
                    for sentence_obj in sentences:
                        if isinstance(sentence_obj, dict) and 'text' in sentence_obj:
                            transcript_parts.append(sentence_obj['text'])

                    transcript = ''.join(transcript_parts)
                    if transcript:
                        logger.info(f'ä»outputè·å–è¯†åˆ«ç»“æœ: {transcript}')
                        return transcript.strip()

                elif isinstance(result.output, dict):
                    transcript = result.output.get('sentence', '') or result.output.get('text', '')
                    if transcript:
                        logger.info(f'ä»å­—å…¸è·å–è¯†åˆ«ç»“æœ: {transcript}')
                        return transcript.strip()

            logger.error(f'DashScope ASRæœªè¿”å›é¢„æœŸç»“æœ: {result}')
            logger.error(f'ç»“æœè¯¦æƒ… - status_code: {getattr(result, "status_code", "N/A")}, output: {getattr(result, "output", "N/A")}')
            return None
            
        except Exception as e:
            logger.error(f'WebMè¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}')
            import traceback
            logger.error(f'é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}')
            return None
            
        finally:
            # æ¸…ç†ä¸´æ—¶WebMæ–‡ä»¶
            time.sleep(1)  # ç»™æ–‡ä»¶ç³»ç»Ÿä¸€ç‚¹æ—¶é—´
            # try:
            #     if os.path.exists(webm_file_path):
            #         os.unlink(webm_file_path)
            #         logger.info(f'æ¸…ç†WebMæ–‡ä»¶: {webm_file_path}')
            # except Exception as cleanup_error:
            #     logger.error(f'æ¸…ç†æ–‡ä»¶å¤±è´¥: {cleanup_error}')
                
    except Exception as e:
        logger.error(f'DashScopeè¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}')
        return None

def evaluate_transcript_accuracy(original_text, spoken_text, context):
    """ä½¿ç”¨é€šä¹‰åƒé—®è¯„ä¼°è½¬å½•å‡†ç¡®æ€§"""
    try:
        evaluation_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„è¯­éŸ³è½¬å½•å‡†ç¡®æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹è¯­éŸ³è½¬å½•çš„å‡†ç¡®æ€§ï¼š

åŸå§‹æ–‡æœ¬ï¼ˆAIä»£ç†è¯´çš„ï¼‰:
"{original_text}"

è½¬å½•æ–‡æœ¬ï¼ˆè¯­éŸ³è¯†åˆ«ç»“æœï¼‰:
"{spoken_text}"

å¯¹è¯ä¸Šä¸‹æ–‡:
{context}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°å¹¶ç»™å‡ºåˆ†æ•°ï¼ˆ0-100åˆ†ï¼‰ï¼š

1. è¯æ±‡å‡†ç¡®æ€§ (40%æƒé‡) - å…³é”®è¯æ˜¯å¦æ­£ç¡®è½¬å½•
2. è¯­ä¹‰å®Œæ•´æ€§ (30%æƒé‡) - æ„æ€æ˜¯å¦å®Œæ•´ä¼ è¾¾
3. ä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§ (20%æƒé‡) - é“¶è¡Œæœ¯è¯­æ˜¯å¦æ­£ç¡®
4. æ•´ä½“å¯ç†è§£æ€§ (10%æƒé‡) - è½¬å½•ç»“æœæ˜¯å¦æ˜“æ‡‚

è¯·è¿”å›JSONæ ¼å¼ç»“æœï¼š
{{
  "overall_score": åˆ†æ•°(0-100),
  "vocabulary_accuracy": åˆ†æ•°(0-100),
  "semantic_completeness": åˆ†æ•°(0-100), 
  "terminology_accuracy": åˆ†æ•°(0-100),
  "comprehensibility": åˆ†æ•°(0-100),
  "grade": "excellent|good|acceptable|poor",
  "issues": ["å…·ä½“é—®é¢˜åˆ—è¡¨"],
  "suggestions": "æ”¹è¿›å»ºè®®"
}}

æ³¨æ„ï¼š
- è½»å¾®çš„è¯­æ°”è¯å·®å¼‚ï¼ˆå¦‚"å—¯"ã€"å•Š"ç­‰ï¼‰ä¸å½±å“è¯„åˆ†
- é‡ç‚¹å…³æ³¨é‡‘é¢ã€æ—¥æœŸã€ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
- å¦‚æœæ ¸å¿ƒä¿¡æ¯å®Œæ•´ï¼Œå…è®¸è¡¨è¾¾æ–¹å¼ç•¥æœ‰ä¸åŒ"""

        response = Generation.call(
            model='qwen-plus',
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„è¯­éŸ³è½¬å½•å‡†ç¡®æ€§è¯„ä¼°ä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°ä¸­æ–‡è¯­éŸ³è½¬å½•è´¨é‡ã€‚è¿”å›æ ‡å‡†JSONæ ¼å¼ã€‚'},
                {'role': 'user', 'content': evaluation_prompt}
            ],
            temperature=0.1,
            max_tokens=800,
            result_format='message'
        )
        
        if response.status_code == 200:
            evaluation_text = response.output.choices[0].message.content
            # å°è¯•è§£æJSON
            try:
                # æå–JSONéƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', evaluation_text, re.DOTALL)
                if json_match:
                    evaluation = json.loads(json_match.group())
                    return evaluation
                else:
                    raise ValueError('æœªæ‰¾åˆ°JSONæ ¼å¼ç»“æœ')
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f'JSONè§£æå¤±è´¥: {e}')
                # è¿”å›é»˜è®¤è¯„ä¼°ç»“æœ
                similarity = calculate_basic_similarity(original_text, spoken_text)
                return {
                    "overall_score": similarity,
                    "vocabulary_accuracy": similarity,
                    "semantic_completeness": similarity,
                    "terminology_accuracy": similarity,
                    "comprehensibility": similarity,
                    "grade": "good" if similarity >= 75 else "acceptable" if similarity >= 60 else "poor",
                    "issues": ["AIè¯„ä¼°è§£æé”™è¯¯"],
                    "suggestions": "å»ºè®®æ£€æŸ¥è¯­éŸ³è¯†åˆ«è®¾ç½®"
                }
        else:
            logger.error(f'è¯„ä¼°APIè°ƒç”¨å¤±è´¥: {response.status_code}')
            return {"error": "è¯„ä¼°å¤±è´¥"}
            
    except Exception as e:
        logger.error(f'è¯„ä¼°è½¬å½•å‡†ç¡®æ€§é”™è¯¯: {str(e)}')
        return {"error": str(e)}

def calculate_basic_similarity(text1, text2):
    """åŸºç¡€ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    if not text1 or not text2:
        return 0
    
    # ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦
    longer = text1 if len(text1) > len(text2) else text2
    shorter = text2 if len(text1) > len(text2) else text1
    
    if len(longer) == 0:
        return 100
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    matches = sum(1 for a, b in zip(shorter, longer) if a == b)
    similarity = (matches / len(longer)) * 100
    
    return int(similarity)

# WebSocketäº‹ä»¶å¤„ç†
@socketio.on('connect')
def handle_connect():
    logger.info('å®¢æˆ·ç«¯è¿æ¥WebSocket')
    emit('connected', {'status': 'connected'})

@socketio.on('disconnect')
def handle_disconnect():
    logger.info('å®¢æˆ·ç«¯æ–­å¼€WebSocketè¿æ¥')

# ====== æµå¼ASRå®ç° ======
class StreamingASRSession:
    """æµå¼ASRä¼šè¯ç®¡ç†"""
    def __init__(self, session_id, client_sid):
        self.session_id = session_id
        self.client_sid = client_sid
        self.recognition = None
        self.start_time = time.time()
        self.is_active = False
        self.results = []
        
    def start_recognition(self):
        """æ ‡è®°ASRä¼šè¯å°±ç»ªï¼ˆå»¶è¿Ÿå¯åŠ¨ï¼Œç­‰å¾…éŸ³é¢‘åˆ°è¾¾æ—¶å†çœŸæ­£å¯åŠ¨ï¼‰"""
        try:
            logger.info(f'ASRä¼šè¯å°±ç»ª: {self.session_id}')
            self.is_active = True
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå¯åŠ¨recognitionï¼Œè€Œæ˜¯åœ¨æ”¶åˆ°éŸ³é¢‘æ—¶å¯åŠ¨
            # è¿™æ ·å¯ä»¥é¿å…DashScope ASRå› ä¸ºæ²¡æœ‰éŸ³é¢‘è€Œè¶…æ—¶
            return True

        except Exception as e:
            logger.error(f'ASRä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}')
            return False

    def process_complete_webm(self, webm_data):
        """æ¥æ”¶å®Œæ•´çš„WebMéŸ³é¢‘æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºPCMåå‘é€åˆ°ASR"""
        if not self.is_active:
            logger.warning('ASRä¼šè¯æœªæ¿€æ´»ï¼Œè·³è¿‡éŸ³é¢‘å¤„ç†')
            return

        try:
            # ğŸ¯ å°†å®Œæ•´çš„WebMæ–‡ä»¶è½¬æ¢ä¸ºPCMæ ¼å¼
            from pydub import AudioSegment
            import io

            logger.info(f'å¼€å§‹è½¬æ¢WebMåˆ°PCM: {len(webm_data)} bytes')

            # ä»å®Œæ•´çš„WebMå­—èŠ‚åˆ›å»ºAudioSegment
            webm_io = io.BytesIO(webm_data)
            audio = AudioSegment.from_file(webm_io, format="webm")

            # è½¬æ¢ä¸º8kHzå•å£°é“16ä½PCMï¼ˆDashScope ASRè¦æ±‚ï¼‰
            audio = audio.set_frame_rate(8000).set_channels(1).set_sample_width(2)

            # è·å–åŸå§‹PCMæ•°æ®
            pcm_data = audio.raw_data

            logger.info(f'PCMè½¬æ¢å®Œæˆ: {len(pcm_data)} bytes, æ—¶é•¿: {len(audio)}ms')

            # ğŸ¯ æ¯æ¬¡æ”¶åˆ°éŸ³é¢‘æ—¶ï¼Œåˆ›å»ºæ–°çš„Recognitionå®ä¾‹å¹¶å‘é€
            # è¿™æ˜¯DashScopeæµå¼ASRçš„æ­£ç¡®ç”¨æ³•ï¼šstart() -> send_audio_frame() -> stop()
            self._process_pcm_with_streaming_asr(pcm_data)

        except Exception as e:
            error_msg = str(e)
            logger.error(f'WebMè½¬æ¢æˆ–ASRå¤„ç†å¤±è´¥: {error_msg}')
            import traceback
            logger.error(f'é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}')

    def _process_pcm_with_streaming_asr(self, pcm_data):
        """ä½¿ç”¨æµå¼ASRå¤„ç†PCMæ•°æ®"""
        try:
            # åˆ›å»ºå›è°ƒå®ä¾‹
            callback = StreamingASRCallback(self)

            # åˆ›å»ºæ–°çš„Recognitionå®ä¾‹
            self.recognition = Recognition(
                model="paraformer-realtime-8k-v2",
                format="pcm",
                sample_rate=8000,
                callback=callback,
                semantic_punctuation_enabled=True,
                max_sentence_silence=800  # å‡å°‘é™éŸ³æ£€æµ‹æ—¶é—´ï¼ŒåŠ å¿«å“åº”
            )

            # å¯åŠ¨è¯†åˆ«
            logger.info(f'å¯åŠ¨æµå¼ASR: {self.session_id}')
            self.recognition.start()

            # åˆ†å—å‘é€PCMæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®æ—¶æµï¼‰
            # DashScopeå»ºè®®æ¯æ¬¡å‘é€3200å­—èŠ‚ï¼ˆ100msçš„8kHz 16bitå•å£°é“éŸ³é¢‘ï¼‰
            chunk_size = 3200  # 100ms of 8kHz 16-bit mono audio
            total_chunks = (len(pcm_data) + chunk_size - 1) // chunk_size

            logger.info(f'å¼€å§‹å‘é€PCMæ•°æ®: {len(pcm_data)} bytes, {total_chunks} chunks')

            for i in range(0, len(pcm_data), chunk_size):
                chunk = pcm_data[i:i + chunk_size]
                self.recognition.send_audio_frame(chunk)

            logger.info(f'PCMæ•°æ®å‘é€å®Œæˆï¼Œç­‰å¾…ASRç»“æœ')

            # åœæ­¢è¯†åˆ«ï¼ˆå‘Šè¯‰ASRéŸ³é¢‘å·²ç»“æŸï¼‰
            self.recognition.stop()

            logger.info(f'æµå¼ASRå¤„ç†å®Œæˆ: {self.session_id}')

        except Exception as e:
            logger.error(f'æµå¼ASRå¤„ç†å¤±è´¥: {e}')
            import traceback
            logger.error(f'é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}')
    
    def restart_recognition(self):
        """é‡ç½®ASRä¼šè¯çŠ¶æ€"""
        logger.info(f'é‡ç½®ASRä¼šè¯: {self.session_id}')
        self.recognition = None
        self.is_active = True
        self.start_time = time.time()
        return True

    def stop_recognition(self):
        """åœæ­¢ASRä¼šè¯"""
        self.is_active = False
        if self.recognition:
            try:
                self.recognition.stop()
            except:
                pass
            self.recognition = None
        logger.info(f'ASRä¼šè¯å·²åœæ­¢: {self.session_id}')

class StreamingASRCallback:
    """æµå¼ASRå›è°ƒå¤„ç†å™¨"""
    def __init__(self, session):
        self.session = session

    def on_open(self):
        logger.info(f'æµå¼ASRè¿æ¥å»ºç«‹: {self.session.session_id}')

    def on_event(self, result):
        """æ¥æ”¶ASRè¯†åˆ«ç»“æœ"""
        elapsed = int((time.time() - self.session.start_time) * 1000)
        logger.info(f'æµå¼ASRç»“æœ ({elapsed}ms): {result}')

        # ä¿å­˜ç»“æœ
        self.session.results.append(result)

        # è§£æç»“æœ
        sentence = result.get_sentence() if hasattr(result, 'get_sentence') else None

        if sentence:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­ç»“æŸ
            is_end = False
            text = ''

            if isinstance(sentence, dict):
                text = sentence.get('text', '')
                is_end = sentence.get('end', False) or sentence.get('sentence_end', False)
            elif isinstance(sentence, list) and len(sentence) > 0:
                # å¯èƒ½æ˜¯å¥å­åˆ—è¡¨
                last_sentence = sentence[-1]
                if isinstance(last_sentence, dict):
                    text = last_sentence.get('text', '')
                    is_end = last_sentence.get('end', False) or last_sentence.get('sentence_end', False)

            if text and is_end:
                logger.info(f'æµå¼ASRå®Œæ•´å¥å­: {text}')
                # å‘é€ç”¨æˆ·æ¶ˆæ¯äº‹ä»¶
                socketio.emit('user_speech_recognized', {
                    'text': text,
                    'timestamp': time.time()
                }, room=self.session.client_sid)

    def on_complete(self):
        logger.info(f'æµå¼ASRè¯†åˆ«å®Œæˆ: {self.session.session_id}')
        socketio.emit('asr_completed', {
            'session_id': self.session.session_id
        }, room=self.session.client_sid)

    def on_error(self, error):
        logger.error(f'æµå¼ASRé”™è¯¯: {error}')
        socketio.emit('asr_error', {
            'session_id': self.session.session_id,
            'error': str(error)
        }, room=self.session.client_sid)

    def on_close(self):
        logger.info(f'æµå¼ASRè¿æ¥å…³é—­: {self.session.session_id}')

# ====== æµå¼ASR WebSocketäº‹ä»¶å¤„ç† ======

@socketio.on('start_streaming_asr')
def handle_start_streaming_asr(data):
    """å¯åŠ¨æµå¼ASRä¼šè¯"""
    try:
        session_id = data.get('session_id', f'asr_{int(time.time())}')
        client_sid = request.sid
        
        logger.info(f'å®¢æˆ·ç«¯è¯·æ±‚å¯åŠ¨æµå¼ASR: {session_id}')
        
        # åˆ›å»ºASRä¼šè¯
        asr_session = StreamingASRSession(session_id, client_sid)
        
        # å¯åŠ¨è¯†åˆ«
        if asr_session.start_recognition():
            # ä¿å­˜ä¼šè¯
            active_asr_sessions[session_id] = asr_session
            
            emit('asr_session_started', {
                'session_id': session_id,
                'status': 'success'
            })
        else:
            emit('asr_session_failed', {
                'session_id': session_id,
                'error': 'å¯åŠ¨æµå¼ASRå¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f'å¯åŠ¨æµå¼ASRä¼šè¯å¤±è´¥: {e}')
        emit('asr_session_failed', {
            'error': str(e)
        })

@socketio.on('send_audio_chunk')
def handle_send_audio_chunk(data):
    """æ¥æ”¶å®Œæ•´çš„WebMéŸ³é¢‘æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºWAVåå‘é€åˆ°ASR"""
    try:
        session_id = data.get('session_id')
        audio_data = data.get('audio_data')  # å®Œæ•´çš„WebMæ–‡ä»¶å­—èŠ‚æ•°ç»„

        if not session_id or not audio_data:
            logger.warning('ç¼ºå°‘session_idæˆ–audio_data')
            return

        # æŸ¥æ‰¾ä¼šè¯
        asr_session = active_asr_sessions.get(session_id)
        if not asr_session:
            logger.warning(f'æœªæ‰¾åˆ°ASRä¼šè¯: {session_id}')
            return

        # è½¬æ¢æ•°æ®æ ¼å¼
        if isinstance(audio_data, list):
            audio_bytes = bytes(audio_data)
        else:
            audio_bytes = audio_data

        logger.info(f'æ”¶åˆ°å®Œæ•´WebMéŸ³é¢‘: ä¼šè¯{session_id}, å¤§å°{len(audio_bytes)} bytes')

        # ğŸ¯ è½¬æ¢å®Œæ•´çš„WebMæ–‡ä»¶åˆ°WAVåå‘é€åˆ°ASR
        asr_session.process_complete_webm(audio_bytes)

    except Exception as e:
        logger.error(f'å¤„ç†WebMéŸ³é¢‘å—å¤±è´¥: {e}')

@socketio.on('stop_streaming_asr')
def handle_stop_streaming_asr(data):
    """åœæ­¢æµå¼ASRä¼šè¯"""
    try:
        session_id = data.get('session_id')
        
        if not session_id:
            logger.warning('åœæ­¢ASRè¯·æ±‚ç¼ºå°‘session_id')
            return
            
        # æŸ¥æ‰¾å¹¶åœæ­¢ä¼šè¯
        asr_session = active_asr_sessions.get(session_id)
        if asr_session:
            asr_session.stop_recognition()
            del active_asr_sessions[session_id]
            
            emit('asr_session_stopped', {
                'session_id': session_id
            })
            
            logger.info(f'æµå¼ASRä¼šè¯å·²åœæ­¢: {session_id}')
        else:
            logger.warning(f'æœªæ‰¾åˆ°è¦åœæ­¢çš„ASRä¼šè¯: {session_id}')
            
    except Exception as e:
        logger.error(f'åœæ­¢æµå¼ASRä¼šè¯å¤±è´¥: {e}')

# ====== ç°æœ‰èŠå¤©å¤„ç† ======

def clean_ai_response_for_tts(ai_text):
    """æ¸…ç†AIå›å¤æ–‡æœ¬ï¼Œç§»é™¤å‚¬æ”¶å‘˜å‰ç¼€ä½†ä¿ç•™æ‰€æœ‰å†…å®¹"""
    # ç§»é™¤"å‚¬æ”¶å‘˜ï¼š"å‰ç¼€å’Œç¼–å·ï¼Œä½†ä¿ç•™æ‰€æœ‰å†…å®¹ä½œä¸ºè¿ç»­æ–‡æœ¬
    cleaned_text = re.sub(r'\d*\.?\s*å‚¬æ”¶å‘˜[ï¼š:]\s*', '', ai_text)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œ
    cleaned_text = ' '.join(cleaned_text.split())
    
    logger.info(f'AIå›å¤æ¸…ç†: åŸæ–‡é•¿åº¦={len(ai_text)}, æ¸…ç†åé•¿åº¦={len(cleaned_text)}')
    logger.info(f'æ¸…ç†åå†…å®¹: "{cleaned_text[:100]}..."')
    
    return cleaned_text.strip()

@socketio.on('update_voice_settings')
def handle_update_voice_settings(data):
    """æ›´æ–°å®¢æˆ·ç«¯è¯­éŸ³è®¾ç½®"""
    try:
        client_sid = request.sid
        voice_settings = data.get('voiceSettings', {})
        client_voice_settings[client_sid] = voice_settings
        logger.info(f'æ›´æ–°è¯­éŸ³è®¾ç½®: {voice_settings}')
    except Exception as e:
        logger.error(f'æ›´æ–°è¯­éŸ³è®¾ç½®å¤±è´¥: {e}')

@socketio.on('chat_message')
def handle_chat_message(data):
    """å¤„ç†èŠå¤©æ¶ˆæ¯å¹¶æµå¼è¿”å›è¿ç»­éŸ³é¢‘"""
    try:
        message = data.get('message', '')
        message_type = data.get('messageType', 'user')
        customer_context = data.get('customerContext', {})
        conversation_history = data.get('conversationHistory', [])
        voice_settings = data.get('voiceSettings', None)

        # å¦‚æœæ¶ˆæ¯ä¸­æ²¡æœ‰è¯­éŸ³è®¾ç½®ï¼Œå°è¯•ä»å…¨å±€è®¾ç½®è·å–
        if voice_settings is None:
            client_sid = request.sid
            voice_settings = client_voice_settings.get(client_sid, {})

        logger.info(f'WebSocketæ”¶åˆ°æ¶ˆæ¯: {message[:50]}... ç±»å‹: {message_type}')

        # å¯¹äºä»£ç†é—®å€™è¯­æˆ–è¯­éŸ³æµ‹è¯•ï¼Œä½¿ç”¨æµå¼TTS
        if message_type == 'agent_greeting' or message_type == 'voice_test':
            logger.info(f'å¤„ç†{message_type}ï¼Œä½¿ç”¨è¿ç»­æµå¼TTS')
            generate_tts_audio_streaming(message, 0, 1, voice_settings)
            return

        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        system_prompt = build_collection_prompt(customer_context, conversation_history)

        # è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆå›å¤
        ai_response, llm_latency = generate_ai_response(system_prompt, message)

        if not ai_response:
            emit('error', {'error': 'ç”ŸæˆAIå›å¤å¤±è´¥'})
            return

        # æ¸…ç†AIå›å¤ - ç§»é™¤"å‚¬æ”¶å‘˜ï¼š"å‰ç¼€ä½†ä¿ç•™å†…å®¹
        cleaned_response = clean_ai_response_for_tts(ai_response)

        # å…ˆå‘é€å®Œæ•´æ–‡æœ¬ç”¨äºæ˜¾ç¤º
        emit('text_response', {'text': cleaned_response})

        # å°†æ•´ä¸ªå›å¤ä½œä¸ºå•ä¸€è¿ç»­éŸ³é¢‘æµå¤„ç†
        logger.info(f'WebSocketè¿ç»­æµå¼å¤„ç†å®Œæ•´å›å¤: {cleaned_response[:50]}...')
        tts_latency = generate_tts_audio_streaming(cleaned_response, 0, 1, voice_settings)

        if tts_latency is None or tts_latency <= 0:
            logger.error('è¿ç»­æµå¼éŸ³é¢‘ç”Ÿæˆå¤±è´¥')
            tts_latency = 0

        # å‘é€å»¶è¿ŸæŒ‡æ ‡åˆ°å®¢æˆ·ç«¯
        emit('latency_metrics', {
            'llm_latency': llm_latency,
            'tts_latency': tts_latency
        })

        logger.info(f'WebSocketå®ŒæˆAIå›å¤è¿ç»­æµå¼å¤„ç†: {cleaned_response[:50]}... (LLM: {llm_latency}ms, TTS: {tts_latency}ms)')

    except Exception as e:
        logger.error(f'WebSocketèŠå¤©å¤„ç†é”™è¯¯: {str(e)}')
        emit('error', {'error': f'å¤„ç†å¤±è´¥: {str(e)}'})

if __name__ == '__main__':
    logger.info('å¯åŠ¨AIå‚¬æ”¶åŠ©æ‰‹QwenæœåŠ¡å™¨ (WebSocketæ”¯æŒ)...')
    logger.info(f'DashScope API Key: {"å·²è®¾ç½®" if DASHSCOPE_API_KEY else "æœªè®¾ç½®"}')
    socketio.run(app, host='0.0.0.0', port=3003, debug=True, allow_unsafe_werkzeug=True)